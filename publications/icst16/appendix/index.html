<html>
<head>
  <title>Symbooglix ICST2016 Appendix data</title>
</head>
  <body>
    <h1>Symbooglix ICST2016 Appendix</h1>
    <p>
      This page contains extra data that could not fit into the ICST2016
      page limit that some may find useful and is noted as being available
      as  &quot;on the project website &quot;.
    </p>

    <h2>Run of Symbooglix snapshots on all benchmarks</h2>
    <p>
      To optimise Symbooglix we ran it on a randomly selected
      subset (10%) of each benchmark suite (SV-COMP and GPU) which we refer
      to as the &quot;training set&quot;. We then
      optimised Symbooglix using only this &quot;training set&quot; taking
      periodic snapshots of the tool and running it on those bencharks.
    </p>
    <p>
      In the paper we provided &quot;quantile plots&quot; for each benchmark
      suite where we ran each Symbooglix snapshot on the &quot;training set&quot;.
      We now show these plots along side quanitle plots where those snapshots
      have been run on all the benchmarks. This was done after we had performed
      optimisation so we did not have access to this data during our optimisation
      process to guide our optimisations.
    </p>

    <img src="sbx_gpu_opt.svg"
         title="Symbooglix snapshots run on the GPU portion of the training set"/>
    <img src="sbx_gpu_all.svg"
         title="Symbooglix snapshots run on all the GPU benchmarks"/>
    <br/>
    <img src="sbx_svcomp_opt.svg"
         title="Symbooglix snapshots run on the SVCOMP portion of the training set"/>
    <img src="sbx_svcomp_all.svg"
         title="Symbooglix snapshots run on all SVCOMP benchmarks"/>
    <br/>

    <h3>Result type counts for GPU</h3>
    <p>
      Here are the classification counts when running on the training set.
    </p>
    <table>
      <tr>
        <th>Snapshot</th>
        <th>Verified</th>
        <th>Bug found</th>
        <th>False alarm</th>
        <th>Unknown</th>
      </tr>
      <tr>
        <td>Baseline</td>
        <td>33</td>
        <td>3</td>
        <td>0</td>
        <td>21</td>
      </tr>
      <tr>
        <td>GlobalDDE</td>
        <td>33</td>
        <td>3</td>
        <td>0</td>
        <td>21</td>
      </tr>
      <tr>
        <td>GotoAssumeLA</td>
        <td>33</td>
        <td>3</td>
        <td>0</td>
        <td>21</td>
      </tr>
      <tr>
        <td>ExprSimpl</td>
        <td>33</td>
        <td>3</td>
        <td>0</td>
        <td>21</td>
      </tr>
      <tr>
        <td>ConstrIndep</td>
        <td>33</td>
        <td>3</td>
        <td>0</td>
        <td>21</td>
      </tr>
      <tr>
        <td>RemSomeRecur</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
      <tr>
        <td>RemSomeDbg</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
      <tr>
        <td>MapConstIdx</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
      <tr>
        <td>MapSymIdx</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
      <tr>
        <td>EffcntClone</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
      <tr>
        <td>SimplSolv</td>
        <td>37</td>
        <td>3</td>
        <td>0</td>
        <td>17</td>
      </tr>
    </table>

    <p>
      Here are the classification counts when running on the whole GPU benchmark suite.
    </p>

    <table>
      <tr>
        <th>Snapshot</th>
        <th>Verified</th>
        <th>Bug found</th>
        <th>False alarm</th>
        <th>Unknown</th>
      </tr>
      <tr>
        <td>Baseline</td>
        <td>260</td>
        <td>29</td>
        <td>0</td>
        <td>290</td>
      </tr>
      <tr>
        <td>GlobalDDE</td>
        <td>260</td>
        <td>29</td>
        <td>0</td>
        <td>290</td>
      </tr>
      <tr>
        <td>GotoAssumeLA</td>
        <td>261</td>
        <td>31</td>
        <td>0</td>
        <td>287</td>
      </tr>
      <tr>
        <td>ExprSimpl</td>
        <td>268</td>
        <td>31</td>
        <td>0</td>
        <td>280</td>
      </tr>
      <tr>
        <td>ConstrIndep</td>
        <td>266</td>
        <td>31</td>
        <td>0</td>
        <td>282</td>
      </tr>
      <tr>
        <td>RemSomeRecur</td>
        <td>301</td>
        <td>35</td>
        <td>0</td>
        <td>243</td>
      </tr>
      <tr>
        <td>RemSomeDbg</td>
        <td>301</td>
        <td>35</td>
        <td>0</td>
        <td>243</td>
      </tr>
      <tr>
        <td>MapConstIdx</td>
        <td>303</td>
        <td>35</td>
        <td>0</td>
        <td>241</td>
      </tr>
      <tr>
        <td>MapSymIdx</td>
        <td>302</td>
        <td>35</td>
        <td>0</td>
        <td>242</td>
      </tr>
      <tr>
        <td>EffcntClone</td>
        <td>300</td>
        <td>35</td>
        <td>0</td>
        <td>244</td>
      </tr>
      <tr>
        <td>SimplSolv</td>
        <td>301</td>
        <td>35</td>
        <td>0</td>
        <td>243</td>
      </tr>
    </table>

    <p>
      Looking at the table above a few interesting things can be observed.
    </p>
    <ul>
      <li>
        Between ConstrIndep and ExprSimpl a regression occurred where two less
        benchmkars were verified
        (<code>CUDA50/6_Advanced/mergeSort/mergeRanksAndIndicesKernel.bpl</code>
        and <code>polybench/stencils/jacobi-1d-imper/kernel0.bpl</code>). These
        benchmarks weren't in the training set.  It is unclear whether these
        are significant though as the two benchmarks that no longer verified
        with ExprSimpl were very close to the timeout in the ConstrIndep
        snapshot. Those two benchmarks in the next snapshot (RemSomeRecur)
        verified in under 22 seconds.
      </li>
      <li>
        Between MapConstIdx and MapSymIdx one less benchmark (<code>shoc/s3d/qssab/kernel.bpl</code>)
        was verified. In the ``MapConstIdx`` this benchmark verified very close to the timeout so it
        is likely that the added overhead pushed this over the timeout.
      </li>
      <li>
        Between MapSymIdx and EffcntClone two less benchmarks (<code>gpgpu-sim_ispass2009/RAY/renderPixel.bpl</code>
        and <code>shoc/devicememory/readGlobalMemoryCoalesced/kernel.bpl</code>) verified. This is a significant
        regression the former used to verify in ~14 seconds and in the newer snapshot hits a timeout and for the
        later it used to verify in ~610 seconds and in the newer snapshot hits a timeout.
      </li>
      <li>
        The results of the final snapshot don't precisely match the results used for the tool comparision in
        the paper. The exact cause of this is unknown and is <a href="https://github.com/symbooglix/symbooglix/issues/16">being investigated.</a>
      </li>
    </ul>

    <h3>Result type counts for SVCOMP</h3>
    <p>
      Here are the classification counts when running on the training set.
    </p>
    <table>
      <tr>
        <th>Snapshot</th>
        <th>Verified</th>
        <th>Bug found</th>
        <th>False alarm</th>
        <th>Unknown</th>
      </tr>
      <tr>
        <td>Baseline</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>374</td>
      </tr>
      <tr>
        <td>GlobalDDE</td>
        <td>9</td>
        <td>35</td>
        <td>0</td>
        <td>330</td>
      </tr>
      <tr>
        <td>GotoAssumeLA</td>
        <td>9</td>
        <td>36</td>
        <td>0</td>
        <td>329</td>
      </tr>
      <tr>
        <td>ExprSimpl</td>
        <td>14</td>
        <td>37</td>
        <td>0</td>
        <td>323</td>
      </tr>
      <tr>
        <td>ConstrIndep</td>
        <td>15</td>
        <td>37</td>
        <td>0</td>
        <td>322</td>
      </tr>
      <tr>
        <td>RemSomeRecur</td>
        <td>15</td>
        <td>39</td>
        <td>0</td>
        <td>320</td>
      </tr>
      <tr>
        <td>RemSomeDbg</td>
        <td>15</td>
        <td>39</td>
        <td>0</td>
        <td>320</td>
      </tr>
      <tr>
        <td>MapConstIdx</td>
        <td>15</td>
        <td>38</td>
        <td>0</td>
        <td>321</td>
      </tr>
      <tr>
        <td>MapSymIdx</td>
        <td>17</td>
        <td>40</td>
        <td>0</td>
        <td>317</td>
      </tr>
      <tr>
        <td>EffcntClone</td>
        <td>17</td>
        <td>40</td>
        <td>0</td>
        <td>317</td>
      </tr>
      <tr>
        <td>SimplSolv</td>
        <td>17</td>
        <td>40</td>
        <td>0</td>
        <td>317</td>
      </tr>
    </table>
    <p>
      One observation about the above table is that between MapConstIdx and RemSomeDbg one
      less bug was found. This was the <code>array-examples/standard_seq_init_true-unreach-call_ground.i_.bpl</code>
      benchmark. In RemSombDbg the bug was found very close to the timeout and so the either
      result inconsistency (due to running on our compute cluster) or slight overhead added
      by MapConstIdx lead to this benchmark timing out for MapConstIdx.
    </p>

    <p>
      Here are the classification counts when running on the whole SVCOMP benchmark suite.
    </p>

    <table>
      <tr>
        <th>Snapshot</th>
        <th>Verified</th>
        <th>Bug found</th>
        <th>False alarm</th>
        <th>Unknown</th>
      </tr>
      <tr>
        <td>Baseline</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>3749</td>
      </tr>
      <tr>
        <td>GlobalDDE</td>
        <td>141</td>
        <td>339</td>
        <td>0</td>
        <td>3269</td>
      </tr>
      <tr>
        <td>GotoAssumeLA</td>
        <td>141</td>
        <td>363</td>
        <td>0</td>
        <td>3245</td>
      </tr>
      <tr>
        <td>ExprSimpl</td>
        <td>200</td>
        <td>390</td>
        <td>0</td>
        <td>3159</td>
      </tr>
      <tr>
        <td>ConstrIndep</td>
        <td>204</td>
        <td>392</td>
        <td>0</td>
        <td>3153</td>
      </tr>
      <tr>
        <td>RemSomeRecur</td>
        <td>204</td>
        <td>393</td>
        <td>0</td>
        <td>3152</td>
      </tr>
      <tr>
        <td>RemSomeDbg</td>
        <td>204</td>
        <td>393</td>
        <td>0</td>
        <td>3152</td>
      </tr>
      <tr>
        <td>MapConstIdx</td>
        <td>204</td>
        <td>393</td>
        <td>0</td>
        <td>3152</td>
      </tr>
      <tr>
        <td>MapSymIdx</td>
        <td>236</td>
        <td>395</td>
        <td>0</td>
        <td>3118</td>
      </tr>
      <tr>
        <td>EffcntClone</td>
        <td>236</td>
        <td>395</td>
        <td>0</td>
        <td>3118</td>
      </tr>
      <tr>
        <td>SimplSolv</td>
        <td>236</td>
        <td>394</td>
        <td>0</td>
        <td>3119</td>
      </tr>
    </table>
      <p>
        A few observations about the above table:
      </p>
      <ul>
        <li>Between EffcntClone and SmplSolv a regression ocurred where one less bug was found. This was the
          <code>ldv-consumption/32_7a_cilled_false-unreach-call_linux-3.8-rc1-32_7a-fs--ecryptfs--ecryptfs.ko-ldv_main1_sequence_infinite_withcheck_stateful.cil.out.c_.bpl</code> benchmark. The bug was previously in aprroximately 450 seconds but for a SmplSolv a timeout was hit.</li>
        <li>
          The results of the final snapshot don't precisely match the results used for the tool comparision in
          the paper. The exact cause of this is unknown and is <a href="https://github.com/symbooglix/symbooglix/issues/16">being investigated.</a>
        </li>
      </ul>

    <h2>Scatter plot of Symbooglix vs Duality</h2>
    <p>
      Below is a scatter plot of bug finding times. Each point is a single
      benchmark in the SV-COMP suite and its position shows the execution time
      of Symbooglix and Duality for that benchmark. For 236 benchmarks
      Symbooglix was faster than Duality and for 350 benchmarks Duality was
      faster than Symbooglix.
    </p>
    <img src="sbx_vs_duality.svg" title="Scatter plot of the execution times of Symbooglix and Duality on benchmarks expected to be incorrect"/>
  </body>
</html>
